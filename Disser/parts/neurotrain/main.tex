%% 
%% Copyright 2007-2024 Elsevier Ltd
%% 
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.3 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.3 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%% 
%% Template article for Elsevier's document class `elsarticle'
%% with harvard style bibliographic references

\documentclass[preprint,12pt,authoryear]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[authoryear,preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times,authoryear]{elsarticle}
%% \documentclass[final,1p,times,twocolumn,authoryear]{elsarticle}
%% \documentclass[final,3p,times,authoryear]{elsarticle}
%% \documentclass[final,3p,times,twocolumn,authoryear]{elsarticle}
%% \documentclass[final,5p,times,authoryear]{elsarticle}
%% \documentclass[final,5p,times,twocolumn,authoryear]{elsarticle}

%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
%% The amsmath package provides various useful equation environments.
\usepackage{amsmath}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

\bibliographystyle{elsarticle-harv} 

% for algorithm from Andrey
\usepackage{algorithm2e}
\RestyleAlgo{ruled}


% FOR COLORED TEXT
\usepackage{color}
% OUR TO-DO COMMAND TO MAKE TEXT BLUE
\newcommand{\TODO}[1]{\textcolor{blue}{#1}}
\newcommand{\OK}[1]{\textcolor{black}{#1}}

% FOR URL-S
\usepackage{hyperref}

% FLOAT OPTION "H" OF FIGURES
\usepackage{float}


%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
%\usepackage{lineno}

% Russian language support
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}


\begin{document}

\title{Быстрая безградиентная максимизация активации\\для нейронов в импульсных нейронных сетях} %% Article title


\begin{itemize}
    \item Мы предлагаем новый метод максимизации активации, основанный на низкоранговом разложении тензорного поезда.
    \item Мы применили наш метод для получения оптимальных стимулов нейронов в импульсной сверточной нейронной сети, что является первым случаем применения безградиентных методов максимизации активации к импульсным нейронным сетям. 
    \item Мы изучили вычисленные оптимальные стимулы послойно на протяжении обучения сети и соотнесли производительность модели с формированием высокоселективных нейронов, аналогично явлениям, наблюдаемым в исследованиях мозга \textit{in vivo}.
    \item Обсуждаются потенциальные применения предложенного метода к биологическим нейронным системам, которые по своей природе лишены прозрачности информационного потока.
\end{itemize}

%% Keywords
нейронная селективность \sep максимизация активации \sep эффективные стимулы \sep объяснимый ИИ \sep нейронное представление \sep безградиентная оптимизация \sep импульсные нейронные сети \sep разложение тензорного поезда \sep генеративные модели
%% keywords here, in the form: keyword \sep keyword

%% PACS codes here, in the form: \PACS code \sep code

%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)



% MAIN TEXT:

\section{Введение}
\label{sec:intro}

Нейроны мозга селективно реагируют на определенные свойства стимулов. Например, нейроны первичной зрительной коры специализируются на простых свойствах видимых изображений, таких как направление движения \cite{HubelWiesel} или цвет \cite{BiPOLES}. В то же время нейроны из высших зрительных областей специализируются на более сложных стимулах, таких как наличие лица в поле зрения \cite{QuianQuiroga2023}. Идентификация специализации живых нейронов включает итеративное изменение свойств стимула (например, изображения, предъявляемого нейрону) для поиска тех, которые вызывают наиболее интенсивный отклик \cite{ponce2019evolving, bardon2022face} - эти подходы известны как \textbf{максимизация активации} (МА). Однако для эффективного решения этой проблемы in vivo необходимо:

\begin{itemize}
    \item Найти оптимальный стимул за минимальное количество итераций и с максимальной точностью.
    \item Исследовать пространство стимулов для поиска множественных оптимумов активации, включая локальные и глобальные оптимумы.
\end{itemize} 

Значение обратных связей в исследованиях интеллекта трудно переоценить – они были всесторонне изучены в области кибернетики \cite{Cybernetics-Ashby} – как в биологических, так и в искусственных системах, и могут считаться необходимыми для того, чтобы система была интеллектуальной. Обратные связи лежат в основе многих подходов обучения с подкреплением (ОП) \cite{Russel-Norvig-AI-modern-approach, Sutton-Barto-RL-book}: существуют так называемые методы обучения с подкреплением на основе человеческой обратной связи (RLHF). Настоящую работу можно рассматривать в этом контексте, только с ИНС в качестве интеллектуального агента, предоставляющего обратную связь. ИНС были выбраны в качестве испытательного стенда для разработки нашей платформы до того момента, когда она станет достаточно быстрой и эффективной для проведения исследований in vivo на живых организмах.

Это особенно важно с учетом этических последствий таких исследований в нейронауке \cite{neuroethics2019, neuroethics2023}. Чем быстрее и качественнее можно получить результаты, тем меньше стресс, оказываемый на исследуемых животных, и тем меньше животных требуется. Для решения этой задачи необходимо разработать и внедрить новые математические методы оптимизации в экспериментальные установки. Это позволит получить наилучшие приближения глобального оптимума активации с минимальным количеством итераций, что означает, что живой объект будет подвергаться воздействию стимулов (таких как изображения или звуки) в течение более короткого периода времени.

В настоящее время широко распространено мнение, что механизмы обучения глубоких нейронных сетей существенно отличаются от механизмов мозга \cite{Brain-is-not-like-ANN,Brain-is-not-like-ANN-2}. Поэтому все прямые сравнения между ними следует проводить с осторожностью.
Однако современные искусственные нейронные сети, несмотря на их разнообразные архитектуры и методы обучения, демонстрируют схожую селективность к сложным стимулам \cite{ANN-neuron-specializations}. Интересно, что, как и в живом мозге, нейроны ИНС могут проявлять селективность к множественным стимулам \cite{goh2021multimodal}.

Поскольку ИНС по сути представляют собой вычислительные графы функций, параметризованных весами слоев, а современное вычислительное оборудование, такое как GPU и TPU, способно эффективно вычислять как значение функции в точке для вывода, так и ее производные по параметрам для обратного распространения, относительно легко ``препарировать'' ИНС \cite{bau2017network} и анализировать ``области ответственности'' каждого искусственного нейрона \cite{Olah-feature-visualization}.

Это делает ИНС идеальными для применения методов максимизации активации, основанных на вычислении градиентов (см., например, \cite{Nguyen2019} для обзора). Безградиентные подходы также применимы к этой проблеме, такие как описанные в \cite{wang2022high}, где было сравнено 15 различных методов для максимизации отклика конкретного нейрона в глубокой ИНС с использованием предварительно обученной генеративно-состязательной сети (GAN) \cite{GANpaper}. Всесторонний сравнительный анализ различных методов оптимизации, включая основанные на генетической и эволюционной оптимизации, был проведен в \cite{TTOpt-paper}. Оригинальный подход, представленный в последней работе, основанный на разложении тензорного поезда (TT-разложение, \cite{TT-Oseledets}), показал значительные преимущества с точки зрения скорости, точности и стабильности (при одинаковом количестве итераций). 

TT-разложение позволяет представить тензор (многомерный массив) в компактной и описательной низкопараметрической форме, которая линейна по размерности. Помимо снижения потребления памяти, TT-формат позволяет выполнять множество алгебраических операций над тензорами: поэлементное сложение и умножение, свертку и интегрирование, решение систем линейных уравнений, вычисление статистических моментов и т.д. – со сложностью, линейной по размерности. TT-формат предоставляет широкий спектр методов для построения суррогатных моделей и эффективного выполнения линейно-алгебраических операций. Недавно было показано в \cite{TTOpt-paper, PROTES-Chertkov-paper, TT-optimization-One-More-Paper-from-Andrei}, что TT-формат также может быть успешно применен для оптимизации черного ящика (безградиентной), и качество решений во многих случаях оказывается выше, чем у методов, основанных на альтернативных безградиентных подходах (например, генетических/эволюционных алгоритмах). Поскольку в рассматриваемой в настоящей работе задаче нет явного доступа к градиентам активации нейронов, мы выбрали этот перспективный тензорный безградиентный подход в качестве основы нашего метода.

Более биологически реалистичной \cite{SNN-biological-relevance} моделью живых нейронных сетей является семейство искусственных импульсных нейронных сетей (ИНС)\cite{SNNs-what-are-overview}, активность которых разворачивается во времени – в отличие от обычных нейронных сетей прямого распространения, которые математически представляют собой просто функции без какой-либо внутренней временной динамики. ИНС представляют собой важный шаг к преодолению разрыва между внутренней работой искусственных и биологических нейронов, поскольку было показано, что время импульсов играет решающую роль в функционировании мозга \cite{Panzeri2001, AndradeTalavera2023}. Учитывая эту биологическую значимость, в настоящей работе мы фокусируемся на максимизации активации нейронов в ИНС. Недавно были изучены внутренние нейронные представления данных в ИНС и проведено их сравнение с представлениями в обычных искусственных нейронных сетях \cite{SNN-neural-representations-2023}.

В нашей работе мы пытаемся решить связанную, но отличную проблему – поиск эффективных стимулов (\textbf{наиболее активирующие изображения, MEI}, названные так в \cite{Inception-what-excites-neurons-most}) специально для нейронов ИНС. Мы стремимся сделать это эффективно, с минимальным количеством итераций и с прицелом на будущие применения к биологическим нейронным сетям. Были предприняты недавние попытки выявить, что возбуждает нейроны в ИНС \cite{Nature-visual-explanation-SNN-gradient-methods} – но, насколько нам известно, в этих попытках использовались методы оптимизации с явным доступом к градиентам (или их суррогатным приближениям) функций активации нейронов. В настоящей работе мы предлагаем сосредоточиться именно на \textbf{безградиентной максимизации активации} для выявления MEI нейронов ИНС – насколько нам известно, это делается впервые.

Эта работа направлена на вклад в развивающуюся область интерпретируемого ИИ и визуализации признаков (\cite{erhan2009visualizing, DNN-visualization-Yosinski, samek2016-evaluating-visualizations-of-what-NN-learns, methods-for-interpreting-NNs,Synthesizing-AM-ANN-generative-Dosovitskiy-et-al,Building-blocks-of-Interpretability}), а также на предоставление полезных инструментов для поиска оптимальных стимулов в биологических экспериментах. Таким образом, она направлена на установление связей между естественным и искусственным интеллектом через единую вычислительную платформу.
Наши вклады в этой статье можно резюмировать следующим образом:
\begin{itemize}
    \item Мы предлагаем новый метод максимизации активации, основанный на низкоранговом разложении тензорного поезда. 
    \item Мы сравнили его с существующими оптимизаторами в библиотеке Nevergrad \cite{Nevergrad} и показали его хорошую сходимость к оптимальным стимулам и превосходную производительность. Мы применили наш метод для изучения MEI импульсной сверточной нейронной сети ResNet, что, насколько нам известно, является первым случаем применения безградиентных методов оптимизации стимулов к ИНС. 
    \item Кроме того, мы изучили вычисленные MEI послойно на протяжении обучения сети и соотнесли производительность модели с формированием высокоселективных нейронов.
\end{itemize}


\section{Предпосылки}\label{sec:Background}

\subsection{Максимизация активации в искусственных и живых нейронных сетях}\label{subsec:AM}

\OK{Наша цель в настоящей работе – оптимизировать активацию конкретного нейрона в искусственной (ИНС) или импульсной (ИНС) нейронной сети путем поиска его наиболее активирующего входного (MEI) стимула. Эта проблема лежит на пересечении современной нейронауки и глубокого обучения (ГО). В ГО известные исследования, такие как XDream \cite{XDream} и Inception Loops \cite{Inception-what-excites-neurons-most}, а также \cite{Synthesizing-AM-ANN-generative-Dosovitskiy-et-al} – продемонстрировали, как генеративные модели могут эффективно отображать латентное пространство стимулов (изображений) для выполнения максимизации активации в ИНС. В нейронауке недавние исследования показали, что можно изучать функцию нейронов в живых организмах, включив генеративную модель, такую как XDream, в петлю обратной связи, которая предъявляет живому субъекту (млекопитающему с хорошо развитой зрительной системой) сгенерированные изображения. В \cite{ponce2019evolving} XDream использовался для идентификации MEI в зрительной коре макак. В \cite{bardon2022face} дальнейшие исследования показали, что MEI не обязательно кодируют точные лица, но также могут кодировать абстрактные объекты (многие из которых выглядят лицеподобными).}

\OK{В настоящей работе мы стремимся внести вклад в быстро развивающуюся область интеграции методов глубокого обучения в нейронауку для решения давнего вопроса о принципах нейронного кодирования. Мы представляем новый подход, который эффективно решает задачу максимизации активации, требуя меньшего количества итераций для сходимости и не требуя информации о структуре базовой системы.}

\OK{С математической точки зрения, интенсивность/частота активации определенного нейрона рассматривается как функция входа, ``подаваемого'' в НС, будь то биологический или искусственный нейрон, MEI определяется как}

\begin{equation}
    \textrm{MEI}_i\,\overset{\textrm{def}}{=} \underset{x\in S}{\arg \max}~A_i
    (x)
\end{equation}

\OK{где $S$ обозначает пространство стимулов, $A_i(x)$ – функция активации $i$-го нейрона в системе. В реальных приложениях $A_i(x)$ всегда измеряется с \textbf{шумом}, что усложняет определение MEI. На практике MEI нейрона вычисляется итеративно. Кроме того, из-за ограничений, накладываемых реальными экспериментами, мы хотим вычислить $\textrm{MEI}_i$ как можно более \textbf{эффективно}, используя минимальное количество итераций. Это важно для экономии времени и поддержания фокуса изучаемого животного в эксперименте \textit{in vivo}, а также для экономии вычислительных ресурсов в эксперименте \textit{in silico}.}


\OK{При изучении биологической системы, такой как мозг животного, у нас нет доступа к ее внутренней структуре, и мы не полностью понимаем, как информация о стимулах передается конкретным нейронам.
Это неформально известно как задача оптимизации функции \textbf{черного ящика} – без явного доступа к градиенту (производным) функции.}

\OK{Методы оптимизации в целом можно условно разделить на:}

\begin{enumerate}
    \item \OK{\textbf{Градиентные} методы, восходящие к Ньютону и его методу, значительно эволюционировали, и вариации стохастического градиентного спуска (SGD) \cite{sgd} сегодня обеспечивают обучение миллионов моделей машинного обучения на основе нейронных сетей по всему миру.}
    
    \item \OK{\textbf{Безградиентные методы}, которые явно не используют информацию о структуре системы. Существует несколько семейств этих методов, включая детерминированные и стохастические. Эволюционные алгоритмы, которые привлекли особое внимание в контексте максимизации активации в последние годы \cite{wang2022high,Gradient-free-AM, XDream}, относятся к этой категории.}
\end{enumerate}


Безградиентные методы естественно более подходят для задачи МА в реальных системах, поскольку активация является функцией типа черного ящика. Конечно, даже если градиент не доступен явно, его можно аппроксимировать конечными разностями, но 1) если ландшафт функции очень ``изрезанный'' – быстро меняющийся – погрешность оценок градиента будет фактически неограниченной; и 2) шум, присутствующий в измерениях значений функции, также будет вносить вклад в погрешность оценок градиента.

В контексте МА совсем недавно такие безградиентные методы были исследованы \cite{Gradient-free-AM} и всесторонне сравнены друг с другом \cite{wang2022high} – как in silico, так и in vivo, причем адаптация ковариационной матрицы (CMA) \cite{CMA} превзошла большинство в обоих случаях.

В настоящей работе мы представляем (впервые) еще одно семейство безградиентных алгоритмов оптимизации для задачи максимизации активации нейронов ИНС – основанные на низкоранговых тензорных разложениях (тензорный поезд, \cite{TT-Oseledets}) оптимизируемой функции. Некоторые из этих методов по сути представляют собой поиск по сетке максимума в массиве, но умный, использующий низкоранговую тензорную природу массива, тем самым достигая экспоненциального ускорения по сравнению с прямым поиском. Другие принадлежат к семейству методов Монте-Карло (МК), которые менее подвержены застреванию в локальных оптимумах за счет стохастических ``прыжков'' по массиву, не обязательно в сторону оптимума. См. соответствующий раздел Методов для самостоятельного введения в эти методы.

\subsection{Импульсные нейронные сети (ИНС)}\label{subsec:SNNs}

\OK{С математической точки зрения обычные ИНС представляют собой просто параметризованные функции, которые преобразуют входные данные для получения выхода – такие ИНС называются сетями \textbf{прямого распространения}. Конечно, область современного глубокого обучения \cite{DL-Fathers-Nature-paper,DeepLearningBook} богата дальнейшими усложнениями этой идеи, например, генеративные модели на основе НС можно назвать недетерминированными функциями, но для целей настоящего обсуждения мы ограничимся основами. В отличие от ИНС, активность биологических НС \textbf{разворачивается во времени}. Живые нейроны постоянно накапливают сигналы от соседних нейронов, с которыми они связаны синаптическими связями, и если общая амплитуда напряжения входящих сигналов (мембранный потенциал, МП) превышает некоторый порог, нейрон ``активируется''. То есть он испускает так называемый \textbf{импульс} – сигнал, форма которого очень специфична (и хорошо изучена \cite{Izhikevich-Spike-model}) из-за биохимической динамики, управляющей его испусканием. Затем импульс распространяется по синапсу к соседним нейронам, чтобы спровоцировать их в свою очередь активироваться. Эта присущая временная природа биологических НС была упущена из виду, когда были представлены первые искусственные модели живых НС, а именно модель нейрона Маккаллока и Питтса 1943 года \cite{Perceptron-McCulloch-Pitts} – далее развитая и реализованная в аппаратном обеспечении и названная перцептроном Розенблаттом \cite{Rosenblatt-Perceptron-1,Rosenblatt-Perceptron-2} в 1957-58 годах. Возможно, из-за аппаратных ограничений того времени реализация функций прямого распространения без какой-либо осцилляторной внутренней динамики была более простой. Изобретение алгоритма обратного распространения ошибки \cite{Backpropagation-Hinton} в 1986 году как полезного метода для обучения таких ИНС (например, для выполнения задач классификации после обучения на примерах данных) еще больше укрепило доминирование ИНС прямого распространения в качестве машинных моделей интеллекта. Именно работа Маасса \cite{Maass-SNN-origins} в 1997 году представила сети искусственных импульсных нейронов как модель биологических НС.}

\OK{Искусственная импульсная нейронная сеть (ИНС), как и биологическая, может рассматриваться как динамическая система, где каждый нейрон является подсистемой со своей собственной внутренней динамикой. Нейроны связаны друг с другом, образуя слои с направленными связями. С течением времени нейрон получает сигналы от своих соседей-источников. Эти сигналы суммируются с весами, обозначающими важность соединений, чтобы получить общее значение ``мембранного потенциала''. Если это значение превышает определенный порог (один из гиперпараметров этой модели), нейрон ``активируется'' – испускает импульс на свой выход. Чтобы эффективно моделировать это в цифровом виде, эта \textbf{динамика дискретизируется}: временная зависимость сигнала кодируется массивом чисел (амплитуд сигнала). Эти массивы по сути являются дискретной моделью того, что называется \textbf{импульсными последовательностями} (термин, используемый и для сигналов в биологических НС). Эта дискретизация не слишком натянута как модель биологической реальности: импульсы в живых нейронах почти ``бинарны'' – МП обычно изменяется почти дискретными шагами.}

% Temporal nature of spike trains in biological NNs is also nearly discrete – spikes have to all arrive at nearly the same time for their contributions to MP to add up.

\OK{В настоящее время существует несколько программных реализаций искусственных ИНС, с по крайней мере двумя выдающимися фреймворками – \textbf{snnTorch} \cite{SNN-Torch-paper} и \textbf{SpikingJelly} \cite{SpikingJelly-paper}, позволяющими выполнять полные процедуры обучения и вывода – эти два были использованы в настоящей работе.}

\OK{При работе с ИНС необходимо ответить на несколько важных вопросов. Во-первых, как входные данные \textbf{кодируются} для обработки ИНС. В нашей работе мы ``предъявляли'' ИНС статические изображения. Для этого существует по крайней мере два подхода \cite{SNN-Torch-paper}: 
\begin{enumerate}
    \itemsep0em 
    \item простое повторение изображения (без изменений) определенное количество кадров (своего рода статическое видео)
    \item показ изображения на определенном количестве кадров, но с изменением его каким-либо шумом.
\end{enumerate}
Мы выбрали первый подход для простоты, поэтому в нашей работе первый импульсный слой \textbf{получал входное изображение, повторенное от 20 до 100 кадров}. Модель, с которой мы работали, по сути является импульсным аналогом ResNet \cite{ResNet-paper}, см. раздел \ref{subsec:SNNs} для дальнейших деталей}.
%In that, the pre-trained CNN plays a role of the visual cortex (already formed and trained to extract visual primitives), while the SNN is somewhat like the associative cortex. For more details on our architecture, see further sections. 

\OK{Второй базовый вопрос: какую именно \textbf{модель импульсного нейрона} использовать? Приведенное выше описание очень схематично, необходимо указать определенную механистическую модель (а именно, электрическую схему) живого нейрона, чтобы динамика такой модели точно аппроксимировала реальность. Для практических применений модель Ходжкина-Хаксли \cite{Hodgkin-Huxley-model}, хотя и является наиболее биологически (физически) точной, слишком сложна для реализации и работы. Поэтому практическим выбором является одна из так называемых \textbf{моделей нейронов с утечкой и накоплением (LIF)}, восходящих к работе Луи Лапика 1907 года \cite{Leaky-Integrate-and-Fire-Lapicque}.}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{Disser/images/neurotrain/SNN-neuron-models.png}
\caption{Модели импульсных нейронов (изображение из \href{https://snntorch.readthedocs.io/en/latest/tutorials/index.html}{онлайн-учебника snnTorch} \cite{SNN-Torch-paper})}
\label{nt:spiking-neuron-models}
\end{figure}    

\OK{В моделях LIF импульсный нейрон представлен как RC-цепь – вместо прямого суммирования входящих (импульсных) напряжений такой нейрон интегрирует входной сигнал во времени с утечкой (из-за R – сопротивления, присутствующего в цепи). Нейрон LIF абстрагируется от формы и профиля выходного импульса: он просто рассматривается как дискретное событие. В результате информация хранится не внутри импульса, а скорее во времени (или частоте) импульсов. Уравнения дискретного времени импульсной динамики для такого нейрона таким образом имеют вид:}

\begin{equation}
    \label{eq:spike-Heaviside}
    S[t] = \Theta(U[t]-U_\text{thr})=\begin{cases}
    1,\quad \text{if }U[t]>U_\text{thr}\\
    0,\quad\text{otherwise}
    \end{cases}
\end{equation}

\OK{где $S[t]$ – интенсивность сигнала (импульса) нейрона в дискретный момент времени $t$, $U[t]$ – МП. При $\Theta(x)$ – ступенчатой функции Хевисайда, нейрон дискретно активируется, если его МП $U[t]$ превышает пороговое значение. Природа RC-цепи нейрона LIF описывается следующим (дискретизированным) уравнением временной динамики для $U[t]$:}

\begin{equation}
    U[t\!+\!1] = \underset{\text{decay}}{\underbrace{\beta U[t]}}+
    \underset{\text{input}}{\underbrace{W X[t\!+\!1]}}-
    \underset{\text{reset}}{\underbrace{S[t] U_\text{thr}}}
\end{equation}

\OK{– в следующий момент времени, $t\!+\!1$, МП задается линейной комбинацией (с обучаемыми весами $W$) входов $X[t]$ от соседних нейронов-источников. Также присутствует некоторое затухание (утечка) и член сброса. При $U_\text{thr}$, установленном, скажем, на 1, единственным \textbf{гиперпараметром}, который остается установить, является скорость затухания, $\beta$.}

\OK{Третий базовый вопрос: как обучать веса соединений нейронов $W$? Чтобы это было решаемо с помощью общих подходов машинного обучения, общая функция потерь активации ИНС на определенных входных данных, $\mathcal{L}_W(\text{input})$, должна быть дифференцируема по параметрам сети, $W$. Проблема в том, что тета-функция Хевисайда в уравнении \ref{eq:spike-Heaviside} не дифференцируема в нуле и имеет нулевую производную везде. Традиционный \cite{SNN-Torch-paper} способ преодолеть это – сгладить $\Theta$, заменив $S[t]$ некоторым \textbf{суррогатом}, $\tilde{S}[t]$ – это может быть любая сигмоидальная функция – обычно логистическая функция или арктангенс. При $\tilde{S}$, являющейся гладкой функцией, веса НС могут обновляться методами градиентной оптимизации. Выбор этой суррогатной функции является еще одним гиперпараметром, который необходимо установить при определении ИНС.} 

В этой работе мы рассматриваем искусственные ИНС как биологически реалистичные модели \cite{SNN-biological-relevance} НС живых мозгов. В типичном биологическом эксперименте внутренние параметры состояния каждого нейрона в живой НС не полностью наблюдаемы – скорее, измерима активность нейрона, возникающая в результате этого состояния и состояния окружающей среды. Эта активность, таким образом, известна как \textbf{функция ``черного ящика''} ответа на стимулы, которым подвергается НС – известны только ее значения в некоторых точках пространства стимулов, а не градиенты. По этой причине мы ограничиваемся рассмотрением этого как проблемы, которая должна быть решена с помощью \textbf{безградиентных методов оптимизации}. Это контрастирует с проблемой МА для неимпульсных ИНС, активность которых явно дифференцируема, и, следовательно, доступны методы оптимизации на основе градиента. Настоящая работа была в значительной степени вдохновлена, в частности, \cite{Gradient-free-AM} – где, хотя рассматриваемые ИНС дифференцируемы, отстаивается именно эта безградиентная парадигма – что можно рассматривать как испытательный полигон для будущих экспериментов \textit{in vivo}, где ИНС заменены живыми НС, активность которых не дифференцируема.


\section{Методы}\label{sec:Methods}

\subsection{Разложение тензорного поезда (TT) и методы оптимизации на основе TT}
\label{subsec:TT-decomposition}

\OK{Как описано выше, наша цель – максимизировать активацию (частоту импульсов) определенных нейронов в ИНС, обученной классифицировать изображения CIFAR-10. Таким образом, мы оптимизируем функцию черного ящика, областью определения которой является $\sim\!10^{2-3}$-мерное латентное пространство признаков изображений (латентные представления изображений доступны с помощью генеративной модели). Если выбрать определенную область этой области и дискретизировать ее (в наших экспериментах латентное пространство изображений было \textbf{дискретизировано в 128-мерный куб с 64 точками на каждой стороне}), этот многомерный тензор может иметь довольно низкий ранг. Обоснование этого заключается в том, что как ИНС (по дизайну), так и биологические НС (из-за ограничений физического мира) не имеют экспоненциальных ресурсов для кодирования всех интересующих мест в этом пространстве. Эта гипотеза о низкоранговой природе активности живых НС является открытым вопросом, который в настоящее время исследуется \cite{low-rank-in-living-neurons-activity,low-rank-in-living-neurons-activity-2}. При таких предположениях методы оптимизации, основанные на низкоранговых тензорных разложениях, могут оказаться очень эффективными.}

\OK{Одним из видов таких разложений, которое превратилось в целую плодотворную область исследований, является разложение тензорного поезда (TT), введенное в 2011 году Оселедцем \cite{TT-Oseledets}. Оно позволяет кодировать низкоранговый многомерный тензор в компактном и удобном формате, используя только полиномиальное (по размерности тензора) количество переменных вместо экспоненциального, требуемого в общем случае. Формат напоминает, если изобразить графически как так называемую тензорную сеть, связанные вагоны поезда, отсюда и название. В этом формате с тензорами можно работать: складывать, умножать, свертывать и т.д. – очень эффективно (по количеству операций и памяти). Все это обеспечивает основу для применения TT-разложения для решения различных линейных и нелинейных уравнений, УЧП и т.д. Из многих возможных применений TT-разложения, помимо задач оптимизации, можно особо отметить ``тензоризацию НС'' \cite{Tensorizing-NNs,liu2022tt} – эффективное построение компактной тензорной аппроксимации нелинейной ИНС. Помимо ускорения задач вывода, это кажется очень связанным с предметом настоящей работы: весь ландшафт значений активации определенного нейрона в ИНС может быть сжат и детально исследован. Если гипотеза о низком ранге справедлива для функций активации живых нейронов, это означало бы, что требуется только полиномиальное (по размерности латентного пространства) количество ``запросов'' (воздействий стимулов) для эффективной аппроксимации ответа живой НС. Это кажется перспективным направлением будущих исследований.}

\OK{Таким образом, существует несколько новых алгоритмов оптимизации, основанных на TT-разложении, где (дискретизированная) оптимизируемая функция $f(x)$ все лучше и лучше аппроксимируется $\tilde{f}_{TT}(x)$ с помощью тензорного поезда на каждой итерации вычисления ее значений в разных точках. В то же время кандидат на оптимум (оптимум $\tilde{f}_{TT}(x)$) быстро находится с использованием структуры TT. Это основная идея метода оптимизации TTOpt \cite{TTOpt-paper}. Другой связанный метод – Optima-TT \cite{Optima-TT-Chertkov}. Также см. \cite{Cichocki-tensors-for-optimization-1,Cichocki-tensors-for-optimization-2} для обзора методов тензорной аппроксимации для оптимизации.}


\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{Disser/images/neurotrain/PROTES_schematic.png}
\caption{Схема метода PROTES.}
\label{nt:PROTES-schematic}
\end{figure}

\OK{Для нашей задачи мы попробовали несколько методов оптимизации на основе TT и обнаружили, что PROTES \cite{PROTES-Chertkov-paper} является наиболее эффективным. PROTES означает ``Вероятностная оптимизация с тензорной выборкой'' – это вероятностный метод оптимизации. Основная идея похожа на идею методов имитации отжига (см. \cite{Simulated-annealing-what-is}) семейства Монте-Карло. В то время как градиентные методы могут застрять в локальных оптимумах, такие методы имеют некоторую вероятность ``выпрыгнуть'' из них, если температура (еще один \textbf{гиперпараметр}) ``частиц, ищущих оптимум'', достаточно высока. Интуиция, стоящая за PROTES, заключается в следующем: дана целевая функция $f(\mathbf{x})$ для минимизации (максимизация выполняется минимизацией $-\!f(\mathbf{x})$), применить к ней следующее монотонное преобразование:} 

\begin{equation}\label{eq:fermi_dirac}
    F[f](x) = \frac1{
        \exp \bigl(
            (f(\mathbf{x}) - y_{\text{min}} - E) / T
        \bigr)
        + 1
    }.
\end{equation}

\OK{(В физике это выражение известно как статистика Ферми-Дирака.) При $y_\text{min}$ – точном или приближенном минимуме $f$, $T>0$ – параметре ``температуры'', и $E$ – некотором пороге ``энергии''. При $F$, являющейся функцией, подобной CDF, можно стабильно найти максимум ее математического ожидания: $\max_\theta \mathbb{E}_{\xi_\theta} F[f](\xi_\theta)$, где семейство случайных величин $\xi_\theta$ имеет параметрическое распределение с плотностью $p_\theta(\mathbf{x})$. Используя трюк REINFORCE \cite{REINFORCE-trick}, можно оценить градиент математического ожидания как}

\begin{equation}
    \nabla_\theta\mathbb{E}_{\xi_\theta}F[f](\xi_\theta)\approx \frac1{M}\sum_{i=1}^M 
    F[f](\mathbf{x}_i)\nabla_\theta \log p_\theta(\mathbf{x}_i)
    \label{expectation-gradient}
\end{equation}

\OK{методом Монте-Карло – $\{\mathbf{x}_i\}_1^M$, являющимися н.о.р. реализациями с.в. $\xi_\theta$. Если удается найти оптимальные значения параметров $\tilde{\theta}$ для $p_\theta$, то ожидается, что $p_{\tilde{\theta}}$ будет иметь пик на максимуме $F[f]$. При ``низкой температуре'' (малых значениях $T$) только несколько членов вносят вклад в сумму (\ref{expectation-gradient}) – а именно те $\mathbf{x}_i$, для которых $f(\mathbf{x}_i)-y_\text{min}<E$ (``частицы низкой энергии'') – для них $F[f]\approx 1$, в то время как для других она близка к нулю. Таким образом, сохраняются только несколько лучших значений выборки. При $p_\theta$, имеющей низкоранговое TT-представление, вышеописанная процедура (выборка и нахождение топ-n значений массива) может быть выполнена быстро и эффективно.}


\subsection{Генеративное моделирование}\label{subsec:generative}


\OK{Задача генеративной модели в конвейере нашего эксперимента – эффективно ``отобразить'' пространство стимулов, предоставляя низкоразмерные латентные координаты для подпространства ``естественных'' изображений в огромном пространстве всех возможных изображений заданного размера. Модели были обучены на CIFAR-10 \cite{CIFAR10-dataset-reference} – наборе данных из 60.000 цветных изображений размером 32x32 пикселя, разделенных на 10 классов (6.000 изображений на класс): 0) самолет, 1) автомобиль, 2) птица, 3) кошка, 4) олень, 5) собака, 6) лягушка, 7) лошадь, 8) корабль, 9) грузовик – см. рис.\thinspace\ref{nt:CIFAR10-classes} в \thinspace\ref{appendix:VAE} для примеров.}

\OK{Мы попробовали два типа генеративных моделей, которые, как известно, хорошо работают в этой задаче:} 

\begin{enumerate}
    \itemsep0em 
    \item Генеративные состязательные сети (GAN) \cite{GANpaper}, в частности их спектрально-нормализованную версию (SN-GAN) \cite{SN-GAN-paper}

    \item Вариационные автокодировщики (VAE) \cite{VAE-vanilla-Kingma-Welling}, в частности их ``дискретизированную'' (векторно-квантованную) версию – VQ-VAE \cite{VQ-VAE-Van-Den-Oord}
\end{enumerate}

\OK{Мы обнаружили, что SN-GAN более подходят в нашей постановке – см. \ref{appendix:VAE} для обзора VQ-VAE и сравнения производительности обеих моделей в нашей задаче. Поэтому в этом разделе мы предоставляем только обзор GAN.}

\OK{Генеративная состязательная сеть (GAN) \cite{GANpaper} состоит из двух НС, генератора $G$ и дискриминатора $D$, играющих в минимаксную игру: $G$ должен изучить распределение, близкое к распределению естественных примеров (изображений), обманывая $D$, что они реалистичны; $D$ должен различать изображения, созданные $G$, от реальных – изучая распределение вероятностей последних (поддерживаемое на пространстве латентных признаков изображений).} 

\OK{Из-за нестабильной природы этой минимаксной игры, в которую играют $G$ и $D$, оригинальные GAN не работают исключительно хорошо на данных, таких как CIFAR-10, поэтому довольно скоро после их введения было предложено много методов регуляризации. Довольно популярным является спектральная нормализация (SN-GAN) \cite{SN-GAN-paper} – с штрафованием весовых матриц слоев таким образом, чтобы их константа Липшица была ограничена сверху. Это простое, но элегантное решение позволило авторам SN-GAN достичь тогдашних современных результатов на CIFAR-10 с точки зрения метрик inception score \cite{GAN-metric-inception-score} и расстояния Фреше (FID) \cite{GAN-metric-Frechet}. Субъективно качество изображений, генерируемых SN-GAN, обученной на CIFAR-10, очень достойное.}

\OK{После проведения нескольких экспериментов мы остановились на использовании SN-GAN с \textbf{латентным пространством размерности 128}, где \textbf{каждое измерение дискретизировано на 64 части}.}

\subsection{Архитектуры ИНС}\label{subsec:SNNs-we-used}

\OK{Прежде чем переходить к экспериментам по максимизации нейрональных ответов \textit{in vivo}, необходимо полностью подготовить и протестировать конвейер такого эксперимента, где живые нейроны заменены некоторой адекватной моделью \textit{in silico}. Наиболее изученным классом таких моделей являются искусственные импульсные нейронные сети (ИНС). Среди всех программных реализаций ИНС были выбраны две наиболее известные и развитые – библиотеки SNNTorch \cite{SNN-Torch-paper} и SpikingJelly \cite{SpikingJelly-paper}.} 

\OK{Особенность импульсных нейронных сетей, как искусственных, так и живых, заключается в том, что импульсы дискретны во времени. Поэтому не сразу очевидно, как обратно распространять ошибку через выход искусственной ИНС, как отмечено в разделе \ref{subsec:SNNs}. В настоящей работе мы используем решение, представленное в \cite{SNN-Torch-paper}, основанное на так называемом методе суррогатного градиента, см. рис.\thinspace\ref{nt:spiking-neuron-models}. То есть локальные правила обучения, распространенные в исследованиях ИНС, не применялись, вместо этого мы обучали сеть с нуля, используя суррогатные градиенты для обратного распространения через импульсные слои.}

\OK{Сначала была реализована и обучена простая сверточная НС на основе SNNtorch для решения задачи классификации изображений из набора данных CIFAR-10 (10 классов цветных изображений 32x32 пикселя). Архитектура модели состояла из ``обычных'' (прямого распространения, не импульсных) сверточных слоев, а также полносвязных слоев. Импульсная природа модели была достигнута путем проецирования входа обычных слоев на \textbf{нейроны с утечкой и накоплением (LIF) с константой затухания мембраны 0,9}. Следует отметить, что существует множество возможных подходов к этому так называемому преобразованию ANN-to-SNN (ANN2SNN) \cite{ANN2SNN-1,ANN2SNN-2}, а также много интересных методов гибридного обучения \cite{ANN-SNN-hybrid-training-1,ANN-SNN-hybrid-training-2}.}

\OK{Полученная ИНС была обучена для достижения \textbf{точности классификации 72\%} на наборе данных CIFAR-10. Это число ниже, чем у современных архитектур прямого распространения (не импульсных), но мы сочли его вполне достаточным для наших предварительных исследований.}

\OK{Затем, чтобы ускорить вывод и увеличить глубину сети, мы интегрировали SpikingJelly \cite{SpikingJelly-paper} в нашу систему. Мы использовали импульсный аналог известной модели ResNet18 \cite{ResNet-paper} для основных результатов настоящей работы, поскольку эта архитектура обеспечивала оптимальный компромисс между глубиной и скоростью вывода. Снова мы {использовали нейроны LIF с константой затухания мембраны, равной 0,9}. Обученная сеть достигла \textbf{точности 86\%} на наборе данных CIFAR10.}

\OK{Активность отдельного нейрона определялась количеством импульсов, которые он производил во время ``экспозиции'' (время было \textbf{дискретизировано на $T=100$ отсчетов для нашей сети на основе SNNtorch и $T=50$ для сети на основе SpikingJelly}). Чтобы учесть импульсную природу сети, каждое изображение, подаваемое на вход модели, копировалось $T$ раз вдоль отдельного измерения тензора. Для каждой эпохи обучения мы сканировали нейронную активность со всех импульсных слоев модели (всего 19 слоев). Активность усреднялась по всем каналам признаков и нормализовалась к диапазону [0,1].}

\subsection{Алгоритмы оптимизации}\label{subsec:optimization-methods}

\OK{
Одним из самых популярных и эффективных программных пакетов для безградиентной оптимизации является фреймворк Nevergrad \cite{Nevergrad}. Реализованные в нем методы использовались в качестве эталонов для нашего исследования. Одной из отличительных особенностей Nevergrad является то, что он реализует методы, которые объединяют несколько подходов оптимизации в один, называемые ``Портфолио''.}

\OK{На основе нашего тестирования мы обнаружили, что базовый метод Portfolio, который объединяет методы 1) стратегии эволюции с адаптацией ковариационной матрицы (CMA-ES), 2) дифференциальной эволюции (DE) и 3) SCR-Hammersley, показал наилучшие результаты в нашей постановке задачи. Мы также сравнили его с другими популярными безградиентными методами, реализованными в Nevergrad, такими как 1) OnePlusOne, 2) NoisyBandit и 3) стохастическая аппроксимация с одновременным возмущением (SPSA), которые все показали худшие результаты по сравнению с Portfolio по умолчанию. Следует отметить, что в \cite{wang2022high} было продемонстрировано, что метод оптимизации на основе CMA превзошел максимальную активацию генетических алгоритмов (на 66\% in silico и 44\% in vivo), которые являются популярным выбором для безградиентных задач МА.}

% здесь бы в будущем вставить таблицу со сравнением

\OK{В качестве альтернативы алгоритмам Nevergrad мы использовали метод оптимизации PROTES \cite{PROTES-Chertkov-paper} (описанный в предыдущих разделах), основанный на низкоранговом разложении тензорного поезда \cite{TT-Oseledets}. В дополнение к базовому PROTES мы применили квантование тензорных мод (преобразование каждого тензорного измерения в новый набор измерений с меньшими модами), что позволило нам достичь лучших результатов по сравнению с базовым методом PROTES.}

Подводя итог, мы сравнили следующие методы оптимизации со следующими \textbf{гиперпараметрами}:
\begin{enumerate}
    \itemsep0em 
    \item (базовый) Случайный поиск
    \item (базовый) Portfolio из пакета Nevergrad
    \item PROTES с $K=10$; $k_{top}=1$
    \item TT-s = PROTES с $K=5$; $k_{top}=1$ + квантование мод
    \item TT-b = PROTES с $K=25$; $k_{top}=5$ + квантование мод
    %\item TT-exp = PROTES с $k=100$; $k_{top}=10$ + квантование
\end{enumerate}

\OK{Для PROTES $K$ обозначает размер выборки кандидатов на оптимум, выбранных из модельной плотности вероятности; а $k_{top}$ – количество лучших кандидатов из $K$, на основе которых параметры этой модельной плотности обновляются на последнем шаге каждой итерации.}

\textbf{Гиперпараметр тензорного ранга} для всех экземпляров PROTES был установлен $r=5$ – то есть каждый ``вагон'' аппроксимации тензорного поезда был тензором ранга 5. Мы провели эксперименты с различными значениями $r$ в близлежащем диапазоне, и наблюдаемые эффекты были похожими, поэтому мы сообщаем результаты с $r=5$.

Схема алгоритма оптимизации на основе TT выглядит следующим образом:

\begin{algorithm}[H]
\caption{Построение НАИ с использованием PROTES}\label{alg:mango}
\footnotesize
\KwData{функция, которая вычисляет активацию целевого нейрона $a = f(x)$ для любого входного изображения $x \in \mathrm{R}^{d_{inp}}$, имеющего $d_{inp}$ пикселей;
    генеративная модель $x = g(z)$, которая создает искусственный вход (изображение) $x$ для предоставленного латентного вектора $z \in \mathrm{R}^{d}$, где $d$ – размерность латентного пространства;
    максимальное количество запросов $m$ (т.е. вычислительный бюджет).}
\KwResult{
    входное изображение $x^{(opt)}$, которое максимизирует активацию.
}

\SetKwFunction{FMain}{loss}
\SetKwProg{Fn}{Function}{:}{}
\Fn{\FMain{$z$}}{
    Генерировать искусственный вход (изображение):
    $
    x = g(z)
    $
    
    Вычислить активацию нейрона:
    $
    a = f(x)
    $
    
    \KwRet $a$
}

Оптимизировать до исчерпания бюджета:
$
z^{(opt)} =
    \mathrm{protes}(\mathrm{loss}, \mathrm{budget}=m)
$

Оптимизатор последовательно генерирует кандидатов для оптимума, выполняет соответствующие запросы к целевой функции и обновляет внутренний тензор ожидания для оптимума на основе возвращенных значений. См. алгоритм 1 с деталями метода оптимизации PROTES в оригинальной работе~\cite{PROTES-Chertkov-paper}.

Генерировать искусственный вход (изображение):
$
x^{(opt)} = g(z^{(opt)})
$

\Return{$x^{(opt)}$}

\end{algorithm}

\subsection{Фреймворк MANGO}\label{subsec:MANGO}

\OK{Мы разработали программный фреймворк для быстрого и точного вычисления НАИ в искусственных нейронных сетях – называемый MANGO (Максимизация нейронной активации посредством безградиентной оптимизации). Мы были особенно заинтересованы в рассмотрении моделей, которые более тесно связаны с механизмами функционирования биологических нейронов, в частности импульсных нейронных сетей (ИНС), поскольку одной из наших основных будущих целей является применение этих методов к живым системам. Однако безградиентные методы оптимизации, которые мы предлагаем в этой статье, также хорошо подходят для классических нейронных сетей.}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{Disser/images/neurotrain/MANGO.png}
\caption{Схема фреймворка MANGO.}
\label{nt:MANGO-framework}
\end{figure}

\OK{В рамках фреймворка можно выбрать набор данных, модель генератора, целевую модель нейронной сети и метод оптимизации. Также можно генерировать поиск НАИ с использованием различных аппаратных бэкендов и сохранять и анализировать результаты.}

Следующий список показывает опции, которые были добавлены в фреймворк:
\begin{enumerate}
    \itemsep0em 
    \item Наборы данных: MNIST \cite{MNIST}, Fashion-MNIST \cite{Fashion-MNIST}, CIFAR10 \cite{CIFAR10-dataset-reference}, Imagenet \cite{Imagenet}
    
    \item Генеративные модели: VQ-VAE \cite{VQ-VAE-Van-Den-Oord}, SN-GAN \cite{SN-GAN-paper}

    \item Классические нейронные сети: AlexNet \cite{AlexNet}, Densenet \cite{Densenet}, VGG \cite{VGG=VeryDeepCNN}, ResNet \cite{ResNet-paper}
    
    \item Импульсные нейронные сети: сверточные сети с поддержкой SNNTorch \cite{SNN-Torch-paper} и импульсный ResNet18 с поддержкой SpikingJelly \cite{SpikingJelly-paper}

    \item Методы оптимизации: случайный поиск, эталоны на основе Nevergrad \cite{Nevergrad} и методы на основе разложения тензорного поезда (TTOpt, PROTES и др.) \cite{TTOpt-paper,PROTES-Chertkov-paper}

    \item бэкенды: CPU (с многопоточностью), GPU, CuPy для моделей на основе ИНС (поддержка предоставлена авторами SpikingJelly \cite{SpikingJelly-paper})
    
\end{enumerate} 
 
Выбранный набор данных используется для обучения сверточной (в нашем случае также импульсной) сети для задачи классификации изображений, а также для обучения сети генератора для задачи сжатия входных данных в эффективное латентное представление. После обучения генератор создает случайные латентные представления, которые преобразуются в изображения и предъявляются сверточной сети. Активация интересующего нейрона измеряется и подается на вход оптимизатору, который создает улучшенный латентный вектор, максимизирующий вероятный ответ изучаемого нейрона. Этот процесс повторяется многократно до сходимости или до исчерпания бюджета запросов оптимизатора.

Код MANGO доступен на GitHub\ \url{https://github.com/iabs-neuro/mango}, включая скрипты обучения и анализа. Полные наборы НАИ будут предоставлены заинтересованным читателям по запросу.

\section{Результаты}\label{sec:results}


\subsection{Производительность методов оптимизации на основе тензорного поезда}\label{subsec:performance}

\OK{Производительность метода оптимизации измерялась с точки зрения активации целевого нейрона в ответ на сгенерированные НАИ.
При большом количестве итераций подходы, основанные на разложении тензорного поезда, продемонстрировали производительность немного ниже, чем базовый метод Nevergrad Portfolio, который был наиболее эффективным среди рассмотренных безградиентных методов, см. рис.\ref{nt:opt-conv} и таблицу \ref{table:am_methods}. Однако при малом или среднем количестве итераций, которые наиболее актуальны в реальных экспериментах, методы на основе TT продемонстрировали сопоставимую или лучшую производительность. Все методы работали значительно лучше, чем базовый случайный поиск.}

\begin{figure}[H]
\centering
\includegraphics[width=0.98\textwidth]{Disser/images/neurotrain/fig4_new.png}
\caption{\textbf{Слева}: Активации нейронов для различных методов оптимизации, усредненные для всех единиц и слоев ИНС. Затененные области представляют стандартные ошибки. \textbf{Справа}: процесс формирования одного НАИ на разных итерациях с соответствующими значениями активации.}
\label{nt:opt-conv}
\end{figure}

В то же время было показано, что метод PROTES в различных модификациях в 3-15 раз быстрее эталона Nevergrad Portfolio, в зависимости от количества итераций, гиперпараметров и свойств изображения. Эта существенная разница делает методы на основе TT очень конкурентоспособными в реальных экспериментальных условиях.

\OK{
Три различных (хотя и связанных) тензорных метода и безградиентный эталон из библиотеки Nevergrad часто сходились на одних и тех же изображениях. Это удивительно, учитывая огромное количество вариаций изображений, выбранных из латентного пространства генератора. Несмотря на то, что различия между НАИ увеличивались в более глубоких слоях (см. раздел \ref{subsec:complexity-and-diversity}), они были очень похожи в ранних слоях сети (см. рис.\thinspace\ref{nt:MEI-gallery-0}, \ref{nt:argmax-gen-comp-color}, \ref{nt:argmax-gen-comp-stripes}).}

\OK{
Это предполагает, что все дискретные алгоритмы оптимизации действительно сходятся к хорошему оптимуму в латентном пространстве генератора. Учитывая результаты из \ref{appendix:VAE}, мы можем распространить эти выводы на общее пространство изображений и заключить, что дискретные методы оптимизации, особенно основанные на разложении тензорного поезда, эффективны в поиске НАИ.}


\begin{table}[ht]

\scriptsize
%\footnotesize

\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
 \hline
 \multicolumn{7}{|c|}{Производительность методов оптимизации} \\
 \hline
~ & Акт. ($10^2$ ит.) &Акт. ($10^3$ ит.) & Акт. ($10^4$ ит.) & Время ($10^2$ ит.) &Время ($10^3$ ит.) & Время ($10^4$ ит.)\\
 \hline
 СП & $0.384\!\pm\!0.001$ & $0.468\!\pm\!0.001$ & $0.537\!\pm\!0.001$ & $10.202\!\pm\!0.033$ & $91.123\!\pm\!0.334$ & $861.266\!\pm\!3.176$  \\
 \hline
 NG & $0.393\!\pm\!0.001$ & $0.574\!\pm\!0.001$ & $\textbf{0.755}\!\pm\!\textbf{0.001}$ & $12.947\!\pm\!0.036$ & $163.45\!\pm\!0.294$ & $1685.464\!\pm\!3.04$ \\
 \hline
 TT & $0.411\!\pm\!0.001$ & $\textbf{0.581}\!\pm\!\textbf{0.001}$ & $0.675\!\pm\!0.001$ & $\textbf{3.267}\!\pm\!\textbf{0.006}$ & $14.456\!\pm\!0.007$ & $219.277\!\pm\!0.289$\\
 \hline
 TT-b & $\textbf{0.418}\!\pm\!\textbf{0.001}$ & $0.579\!\pm\!0.001$ & $0.662\!\pm\!0.001$ & $3.321\!\pm\!0.005$ & $\textbf{14.283}\!\pm\!\textbf{0.026}$ & $383.852\!\pm\!0.489$\\
 \hline
 TT-s & $0.392\!\pm\!0.001$ & $0.564\!\pm\!0.001$ & $0.679\!\pm\!0.001$ & $3.724\!\pm\!0.005$ & $15.236\!\pm\!0.007$ & $\textbf{127.406}\!\pm\!\textbf{0.173}$\\
 \hline
\end{tabular}
\caption{Средние НАИ-связанные активации и время вычисления НАИ для различных безградиентных методов оптимизации. Методы: RS – случайный поиск; NG – базовый Nevergrad Portfolio; TT, TT-b, TT-s – варианты оптимизации на основе разложения тензорного поезда}
\label{table:am_methods}
\end{table}


\subsection{Возникновение и динамика нейронных специализаций}\label{subsec:emergence-and-dynamics}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{Disser/images/neurotrain/am_u52_layer1.1.sn2.png}
\caption{Нейрон «розовая лошадь» 52 из LIF слоя 1.1 импульсной ResNet18. Изображения были получены с использованием методов на основе разложения тензорного поезда. Числа показывают активации целевого нейрона на НАИ}
\label{nt:argmax-pink-horse}
\end{figure}


\OK{Для изучения динамики возникновения НАИ и их содержания были проанализированы оптимальные стимулы для импульсных слоев ИНС ResNet18. Нас интересовали максимальные нейронные активации, достигнутые в процессе оптимизации, и их соответствие объектам в обучающих изображениях. Обучение проводилось в течение 1000 эпох, состояния сети записывались после каждой эпохи. Были выбраны девять импульсных слоев, равномерно распределенных между входным и выходным слоями, в каждом слое для построения НАИ было отобрано 64 нейрона. Активация нейронов максимизировалась для всех 576 рассматриваемых нейронов последовательно путем поиска наиболее возбуждающего визуального входа с использованием трех вариантов метода PROTES на основе разложения тензорного поезда. В качестве генеративной модели использовался SN-GAN (также использовался генератор изображений VQ-VAE, сравнение между этими двумя моделями приведено в \ref{appendix:VAE})}.


\OK{Важным аспектом при анализе НАИ является взаимосвязь между выявленными нейронными специализациями и паттернами, которые сеть изучила из набора данных.}
\OK{Для изучения этого полученные НАИ затем подавались в сеть для определения, был ли целевой нейрон специфичен для определенного класса изображений. Для количественной оценки селективности нейронов к конкретным классам мы запустили сеть на НАИ и получили вероятности классов модели. Можно утверждать, что более равномерное распределение вероятностей классов соответствует более абстрактным и общим изображениям. И наоборот, высокая вероятность для конкретного класса указывает на наличие соответствующего паттерна в НАИ. Для каждого НАИ была вычислена энтропия конечного распределения вероятностей классов и нормализована относительно максимально возможной энтропии (для равномерного распределения по 10 классам). Существуют также другие заметные меры селективности нейронов в ИНС \cite{SNN-neuron-selectivity-Chevallier}.}

%The procedure was repeated 100 times to account for single-trial spike variability inevitably present in SNN.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{Disser/images/neurotrain/entropies.jpg}
\caption{Распределение нейронов по энтропии вероятностей предсказания классов (меньшая энтропия = больше специализация на классе). \textbf{Слева}: ранняя стадия обучения (эпоха №5), \textbf{справа}: поздняя стадия обучения (эпоха №700).}
\label{nt:layer_spec}
\end{figure}

\OK{
Результаты показаны на рис.\thinspace\ref{nt:layer_spec}. Можно видеть резкий пик НАИ с низкой энтропией в последних импульсных слоях обученной модели, что соответствует высокой уверенности модели в присвоении конкретных классов НАИ. Даже в середине сети большинство нейронов уже специфически связаны с определенным классом. Напротив, на ранней стадии обучения модель не показывает какой-либо специфичности НАИ для конкретного класса.
}

Следуя за процессом формирования НАИ, мы наблюдали, что он часто состоял из двух различных стадий. Сначала оптимизация искала в латентном пространстве изображений почти случайным образом, при этом уверенность классификации сети (отрицательная энтропия распределения вероятностей выходных классов) была низкой. Затем процесс максимизации активации сходился к некоторому НАИ, а дальнейшая оптимизация только уточняла его. Переход от первой стадии ко второй был отмечен резким увеличением уверенности классификации сети, особенно на НАИ, которые напоминают обучающие изображения четко различимых классов (см. рис. \ref{nt:opt-conv}).

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{Disser/images/neurotrain/fraction-of-selective-neurons.png}
\caption{{Доля селективных нейронов. Горизонтальная ось - номер эпохи, вертикальная ось - слои (ниже = глубже). \textbf{Слева}: порог активации – импульсы $\geq$75\% времени экспозиции; \textbf{Справа:} дополнительно максимальная вероятность класса $\geq$75\%, с максимальным классом, стабильным при различных методах максимизации.}}
\label{nt:fraction-of-selective-neurons}
\end{figure}

\OK{Примечательно, что три используемых метода на основе разложения тензорного поезда часто сходились к похожим изображениям. Учитывая высокую сложность базового ландшафта потенциальных НАИ, мы использовали следующий составной критерий селективности нейронов по отношению к конкретному классу:}
\begin{itemize}
    \itemsep0em 
    \item «Стабильность»: все три метода максимизации сходились на изображениях одного и того же класса (класс НАИ определялся по максимальной вероятности на выходе сети).
    \item «Уверенность»: вероятность лучшего класса составляла не менее 75\%.
    \item «Активация»: активность целевого нейрона составляла не менее 75\% от максимально возможной. Здесь активность вычислялась простым суммированием количества импульсов по всем каналам для данного нейрона.
\end{itemize}

\OK{
Как и ожидалось, подавляющее большинство специализированных нейронов было обнаружено в конечных слоях модели, где НАИ содержали наиболее сложные паттерны (рис. \ref{nt:fraction-of-selective-neurons}, см. \ref{appendix:MEI-gallery} для характерных примеров НАИ). Однако, что удивительно, мы также обнаружили значительное количество нейронов, связанных с определенным классом, в промежуточных слоях, составляющих до 40\% от общего числа. Неожиданно, высокоселективные нейроны были обнаружены уже в третьем импульсном слое (рис.\thinspace\ref{nt:argmax-pink-horse}).
}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{Disser/images/neurotrain/emergence-of-specializations.png}
\caption{
Возникновение нейронных специализаций. Горизонтальная ось – глубина слоя (увеличивается слева направо). \textbf{Слева}: распределение (по нескольким запускам) номеров первых эпох, когда появляется сильный НАИ (импульсы нейрона $\geq$75\% времени экспозиции). \textbf{Справа}: нормализованная активация от НАИ (планки погрешностей – 95\% доверительный интервал).}
\label{nt:image-acts}
\end{figure}

\OK{
По мере обучения модели количество высокоспециализированных нейронов увеличивается. Для количественной оценки «качества» НАИ мы вычислили усредненный нейронный отклик, вызванный НАИ, по слоям и эпохам обучения. Результаты показаны на рисунках \ref{nt:image-acts} и \ref{nt:activation-dynamics}. В целом, НАИ-связанная активность в последних слоях увеличивалась с обучением, хорошо коррелируя с производительностью модели (рисунок \ref{nt:activation-dynamics} справа). Для промежуточных слоев НАИ-связанная активность достигала плато и даже могла показывать небольшое снижение на поздних эпохах. Мы обсуждаем эти эффекты и сравниваем их с теми, которые происходят в неимпульсной модели ИНС в \ref{appendix:ANN}. Однако мы наблюдали аномально позднее развитие НАИ перед конечными слоями нейронной сети (слой 3.1). Эти НАИ приводили к более слабой активации целевых нейронов (рисунок \ref{nt:image-acts} справа). Мы полагаем, что это снижение может быть связано с продолжающейся реструктуризацией НАИ в этих слоях, поскольку они действуют как «шлюз» к более сложным специализированным слоям. Также возможно, что недостаточный бюджет оптимизации не позволил получить «истинные» НАИ этих нейронов.
}


\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{Disser/images/neurotrain/activation-dynamics.png}
\caption{{\textbf{Слева}: динамика активаций нейронов в ответ на их НАИ. \textbf{Справа}: то же самое для первых 100 эпох с точностью классификации модели на тех же осях.}}
\label{nt:activation-dynamics}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=1.0\textwidth]{Disser/images/neurotrain/by_1-2-many_specializations_and-bird-ships.jpg}
\caption{\textbf{Слева:} доли неспециализированных, стабильных (селективных только к 1 классу) и лабильных (селективных к 2 или более) нейронов в зависимости от глубины слоя. \textbf{Справа:} осциллирующие НАИ лабильного \textbf{\textit{нейрона «птица-корабль»}}, \textbf{строки}: эпохи № 40, 100, 300 обучения; \textbf{столбцы:} результаты, полученные методами оптимизации TT, TT-S, TT-B. Подписи над каждым НАИ содержат информацию о преобладающем классе согласно результатам классификации, вероятности и активации нейрона в процентах от максимально возможной.
}
\label{nt:labile-neurons-distr-and-bird-ship}
\end{figure}

\OK{
Затем мы рассмотрели вопрос о «лабильных» нейронах, которые изменяют свою селективность по мере обучения. Для этого мы отслеживали нейроны, которые соответствовали критериям специализации более чем на одном классе во время обучения. Эти нейроны интересны тем, что помогают понять механизмы формирования «узких» специализаций. Они часто специализируются на абстрактных концепциях, которые могут использоваться в качестве строительных блоков для классификации более сложных паттернов. Пример НАИ такого нейрона можно увидеть на рис.\thinspace\ref{nt:fraction-of-selective-neurons}, справа. Он селективно активируется на вытянутых, диагонально расположенных структурах на сине-зеленом фоне. В этой области латентного пространства вложения птиц и кораблей похожи, что отражается в классификации нейрона на разных этапах обучения. Такие лабильные нейроны практически отсутствовали в начальных слоях сети (см. рис.\thinspace\ref{nt:fraction-of-selective-neurons}, слева), но их доля возросла примерно до 40\% в конечном слое. Это может указывать на высокую степень изменчивости специализации конечных слоев и их способность к быстрой реконфигурации при получении новых обучающих данных, опираясь на информацию от нейронов в более ранних слоях. В частности, это поддерживает эмпирический принцип, лежащий в основе резервуарных вычислений, который заключается в обучении только выходных слоев, позволяя внутренним слоям идентифицировать сложные признаки в наборе данных \cite{Tanaka2019}.
}

\subsection{Сложность и разнообразие специализаций}\label{subsec:complexity-and-diversity}

\OK{На основе наших наблюдений, энтропия предсказаний вероятностей классов моделью на НАИ уменьшается с глубиной слоя и количеством эпох обучения, что указывает на постепенное формирование нейронами более утонченных специализаций, связанных с классами.}

\OK{В целом, как и ожидалось, нейроны ранних слоев максимально активируются простыми геометрическими признаками или цветовыми паттернами, в то время как нейроны в более глубоких слоях показывают селективную активацию для того или иного класса (см. рис.\thinspace\ref{nt:MEI-gallery-0}, \ref{nt:MEI-gallery-1}, \ref{nt:MEI-gallery-2} для примеров).}

\OK{НАИ, полученные с использованием различных методов оптимизации, содержат одни и те же паттерны различной степени сложности, но не идентичны друг другу. Для оценки вариабельности полученных изображений мы вычислили средние евклидовы и косинусные расстояния между латентными векторами, соответствующими НАИ (использовался генератор SN-GAN).}

\OK{Вариабельность возможных изображений в последних слоях импульсной ResNet18 оказалась больше, чем в первых (см. рис.~\ref{nt:image-dists}). Средние расстояния между сгенерированными НАИ в последнем импульсном слое оказались примерно на 20\% выше, чем в первом, выявляя значительные трансформации ландшафта специализаций нейронов. Возможное объяснение этого эффекта заключается в том, что существует множество способов, которыми сложная концепция, такая как лошадь, может быть реализована в изображении. Нейронные сети учатся нелинейно проецировать пространство данных во время обучения, одновременно отождествляя некоторые его области друг с другом \cite{nnmap}. Таким образом, количество локальных максимумов в общем пространстве изображений растет с увеличением специализации нейронов.
В то же время этот эффект, измеренный через косинусное расстояние, становился слабее во время обучения модели (см. рис.\thinspace\ref{nt:image-dists}, справа).
}

\begin{figure}[H]
\centering
\includegraphics[width=1.0\textwidth]{Disser/images/neurotrain/MEI-distances.jpg}
\caption{Разнообразие специализаций с точки зрения расстояний между НАИ в латентном пространстве. \textbf{Слева}: евклидовы расстояния. \textbf{Справа}: косинусные сходства для разных слоев (глубина слоя увеличивается вправо) на разных этапах обучения.}
\label{nt:image-dists}
\end{figure}

\OK{Растущее разнообразие НАИ сопровождается увеличением активаций, которые они производят на соответствующих нейронах(~\ref{nt:image-acts}). Это может быть объяснено более тонкой специализацией нейронов в более глубоких слоях в соответствии с результатами из раздела \ref{subsec:emergence-and-dynamics}. В то же время провалы в максимальных значениях активации, наблюдаемые в слое 3.1, воспроизводятся обоими типами генераторов (GAN и VAE) и показывают, что общая картина может быть более сложной.}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{Disser/images/neurotrain/complexity-by-compression.jpg}
\caption{Сложность НАИ, измеренная как размер сжатого файла (сжатие JPEG и ZIP).}
\label{nt:complexity-by-compression}
\end{figure}

\OK{По мере обучения сети содержание НАИ становится более сложным. Вычисление точной меры сложности изображения является сложной задачей, для которой было разработано несколько дополняющих друг друга подходов \cite{Chikhman2012}. Однако можно получить общее представление об этой мере, используя простые эвристики, основанные на алгоритмах сжатия информации. Этот подход оказался успешным при оценке энтропии и взаимной информации \cite{Baronchelli2005}, а также количественной оценке уровня сознания \cite{Casali2013}. Для количественной оценки визуальной сложности сгенерированных НАИ мы вычислили коэффициенты сжатия оптимизированных изображений в форматах \texttt{.jpg} и \texttt{.zip} и сравнили их с размером исходных изображений (см. рис.\thinspace\ref{nt:complexity-by-compression}). Оба метода сжатия дали похожие результаты: по мере обучения сети сложность НАИ в первых слоях увеличивается, постепенно достигая устойчивого состояния. Мы полагаем, что это связано с быстрым формированием более сложных специализаций в ранних слоях сети (рис.\thinspace\ref{nt:image-acts}, см. раздел \ref{subsec:neuronal-selectivity} в Обсуждении).}


\section{Обсуждение}\label{sec:Discussion}

\subsection{Селективность нейронов в импульсных сверточных сетях}\label{subsec:neuronal-selectivity}

\OK{
Мы отслеживаем свойства селективности нейронов на протяжении всего процесса обучения модели и наблюдаем НАИ различной сложности с более утонченными признаками, соответствующими более глубоким слоям, в соответствии с принципами, показанными как в ИНС \cite{Bengio-transferable-features,AlexNet,erhan2009visualizing}, так и в зрительной коре животных \cite{DiCarlo2012, QuianQuiroga2023}.
}

\OK{
Нейроны из промежуточных слоев часто селективны к неоднозначным изображениям, потенциально полезным для различения нескольких классов (см. рис.\thinspace\ref{nt:MEI-gallery-1} (слева) для примеров). Этот эффект может обеспечить потенциальную основу для сложных специализаций изображений. Например, было показано, что абстрактные представления признаков естественным образом возникают в нейронных сетях, обученных выполнять несколько задач \cite{Johnston2023}.
}

\OK{
Влияние различных сенсорных модальностей на зрительную кору у животных не является незначительным, потенциально приводя к сложным специализациям нейронов \cite{Pennartz2023}. Например, нейроны в зрительной области V2 могут быть селективными к «натуралистичным» текстурным стимулам \cite{Movshon2014}. Нейроны, селективные к сложным вокальным стимулам, также были обнаружены в первичной слуховой коре \cite{MontesLourido2021}. Эти находки совпадают с формированием сложных специализаций в ранних слоях, как продемонстрировано в этой работе (см. рис.\thinspace\ref{nt:fraction-of-selective-neurons}).
}

\OK{
Примечательно, что нейронные специализации в нашей постановке формируются рано в процессе обучения, как показано на рис.\thinspace\ref{nt:fraction-of-selective-neurons} – это явление также наблюдалось в неимпульсных ИНС \cite{critical-learning-periods}. Нейроны модели способны формировать сложные представления, выраженные в активности нейронов, после нескольких экспозиций к набору данных. Это происходит как в глубоких, так и в промежуточных слоях, даже несмотря на то, что качество распознавания еще не достигло плато на этом этапе обучения. Это согласуется с данными о раннем формировании специализированных нейронов в биологических нейронных сетях, полученными в экспериментах \cite{Sotskov2022}. Такое поведение может указывать на то, что сложные нейронные представления формируются быстро, а затем сеть «перестраивает» веса с течением времени для оптимальной передачи информации от этих высокоспециализированных нейронов к выходным слоям.
}

Полученные результаты предполагают, что реконфигурация нейронной специализации является фундаментальным процессом как в биологических, так и в искусственных нейронных сетях. Кроме того, в этих системах наблюдается гетерогенность в селективности нейронов: некоторые нейроны сохраняют свою специализацию по мере обучения, в то время как специализации других более лабильны. В биологических сетях было показано, что нейроны с более сильной селективностью, как правило, более стабильны, что поддерживает гипотезу логарифмически-динамического мозга \cite{log_dynamic}. В будущей работе мы планируем исследовать, как это поведение связано с силой специализации, а также изучить веса связей между искусственными нейронами с конкретными специализациями, чтобы лучше понять их влияние друг на друга и проследить развитие сложных специализаций.

\subsection{Ограничения текущей работы и направления будущих исследований}\label{subsec:limitations}

Как показывают наши результаты, качество вычисленных НАИ существенно зависит от используемого генератора изображений. В будущем мы планируем расширить MANGO, добавив поддержку новых генераторов, которые хорошо подходят для задачи поиска НАИ (то есть сочетающих дискретную структуру латентного пространства с высокой правдоподобностью сгенерированных изображений). Представляет интерес рассмотреть популярное направление дискретных GAN \cite{Taming-transformers}.

Важный исследовательский вопрос остается о паттернах и механизмах формирования нейронных специализаций с течением времени. Хотя в настоящей работе мы исследовали НАИ для уже обученной сети, представляет большой интерес возможность отслеживать их эволюцию в процессе обучения. Мы планируем добавить эту функцию в фреймворк MANGO в будущем.

Одним из наиболее интересных и потенциально плодотворных применений наших инструментов является максимизация нейронной активации \textit{in vivo}. Поскольку биологические нейроны имеют неустранимое минимальное время ответа на стимул, исследование алгоритмов, направленных на снижение бюджета оптимизатора, необходимого для генерации НАИ, кажется особенно актуальным. В частности, мы планируем расширить MANGO дополнительными методами оптимизации на основе разложения тензорного поезда, специально оптимизированными для этой задачи.

Параллельно с селективностью одиночных нейронов, нейронные вычисления могут быть описаны низкоразмерными многообразиями активности, которые возникают из коллективной, скоординированной импульсной активности многочисленных нейронов. Этот тип кодирования информации в мозге известен как популяционное кодирование и имеет долгую и богатую историю в нейронауке \cite{Panzeri2015, Gallego2017}. Недавно популяционное поведение искусственных нейронов в больших языковых моделях (БЯМ) получило особое внимание, потенциально приводя к появлению новой области инженерии представлений \cite{repE}. Насколько нам известно, взаимосвязи между оптимальными стимулами для одиночных нейронов и их вкладом в популяционное кодирование еще не были проанализированы, и мы считаем это одним из важных направлений для будущей работы.

Недавно стало очевидно, что активность отдельных компонентов может существенно влиять на работу большой нейронной сети. Исследователи, работающие с БЯМ, недавно продемонстрировали, что манипулирование активностью одного специализированного искусственного нейрона может привести к значительным изменениям в поведении БЯМ \cite{templeton2024scaling}. В то же время известно, что стимуляция небольшого количества нейронов (или даже одного нейрона в некоторых случаях) в биологической нейронной сети может вызвать высокую активность всего ансамбля, потенциально влияя на поведение животного \cite{AlejandreGarca2022}. Учитывая эти свидетельства важности отдельных вычислительных единиц, было бы интересно исследовать влияние подавления или возбуждения одного нейрона на производительность модели и соотнести это со свойствами НАИ этого нейрона. Мы планируем провести такой анализ в будущей работе.


\subsection{Максимизация активации в ИНС и объяснимый ИИ}\label{subsec:explainable-AI}

\OK{Проблема максимизации нейронной активации в искусственных нейронных сетях (ИНС) активно исследуется в области визуализации и интерпретации ИНС \cite{erhan2009visualizing,samek2016-evaluating-visualizations-of-what-NN-learns,DNN-visualization-Yosinski,methods-for-interpreting-NNs}, подобласти более общей области объяснимого ИИ. Для успешной интеграции решений на основе ИНС в критические системы, такие как медицинская и юридическая практика, необходимо иметь понятную человеку интерпретацию процесса принятия решений алгоритмом. В последние годы методы, основанные на визуализации вычислительного графа, функции потерь, пространства параметров определенных слоев или даже отдельных нейронов \cite{Bau-2020-role-of-individual-neurons}, приобрели популярность в создании интерпретируемых моделей глубокого обучения. Эти методы позволяют исследователям лучше понимать внутреннюю работу нейронных сетей.}

\OK{Недавний обзор \cite{matveev2021overview} методов визуализации для раскрытия нейронных стимулов охватывает некоторые из наиболее успешных подходов: метод максимизации активации \cite{erhan2009visualizing}, метод Grad-CAM \cite{selvaraju2017grad}, метод послойного распространения релевантности (LRP) \cite{methods-for-interpreting-NNs}, а также метод интегрированных градиентов \cite{sundararajan2017axiomatic}. Еще одна выдающаяся недавняя работа \cite{goh2021multimodal} демонстрирует богатый диапазон стимулов, которые максимизируют активацию различных нейронов в ИНС. Эта работа также демонстрирует эффект мультимодальной селективности в искусственных нейронах.}

\OK{Однако все вышеупомянутые методы максимизации активации в основном основаны на автоматическом дифференцировании: формируется вычислительный граф ИНС, позволяющий вычислять градиенты с минимальной вычислительной сложностью. Это позволяет использовать градиентный спуск (подъем) и его стохастические модификации. Градиентные методы явно неприменимы к живым системам, поэтому необходимо использовать либо безградиентные методы, такие как генетические алгоритмы (как в \cite{XDream}), либо другие методы, использующие локальные аппроксимации градиентов, такие как эволюционные алгоритмы, или альтернативные подходы, включая методы оптимизации на основе разложения тензорного поезда.} 

%Notably genetic and evolutionary algorithms were considered in \cite{wang2022high} for activation maximization in ANNs.

\OK{Наши результаты, представленные в разделе \ref{subsec:performance}, указывают на то, что методы оптимизации на основе разложения тензорного поезда превосходят Portfolio от Nevergrad \cite{Nevergrad} в нашей постановке задачи. Portfolio представляет собой сбалансированную смесь методов оптимизации, которая включает CMA-ES \cite{CMA}. Было показано \cite{wang2022high}, что CMA-ES превосходит другие генетические алгоритмы в задаче максимизации активации. Это позволяет осторожно заключить, что методы оптимизации на основе разложения тензорного поезда более подходят для задач максимизации активации, хотя это утверждение требует дальнейшего точного исследования.}


\subsection{Возможные применения максимизации активации на основе разложения тензорного поезда в биологических нейронах}\label{subsec:in-vivo}

\OK{Современные методы оптической визуализации позволяют отслеживать активность сотен нейронов в их временной динамике in vivo. Максимизация активации множественных нейронов, как индивидуально, так и в группах, может быть мощным инструментом для анализа функции нервной системы на клеточном уровне, проливая свет на формирование и распространение когнитивных специализаций в живых нейронах. Однако дизайн биологических экспериментов накладывает серьезные ограничения на применимость методов оптимизации. Эти методы должны быть быстрыми, точными и способными настраиваться на новые данные. Мы ожидаем, что наши методы на основе разложения тензорного поезда будут обладать всеми этими свойствами.}

\OK{Отдельной проблемой является поиск оптимальных стимулов в невизуальных областях, таких как пространство звуков или запахов, которые могут быть гораздо более релевантными для изучаемого животного, чем для людей \cite{Arakawa2008}. Мы также надеемся получить некоторое понимание применимости методов на основе разложения тензорного поезда в этих латентных пространствах стимулов.}

\OK{Исследования селективности нейронов в импульсных нейронных сетях представляют большой интерес, поскольку они раскрывают механизмы работы функциональных единиц в нейронной сети с использованием биологически правдоподобной модели. Использование импульсных искусственных нейронов делает анализ в этом исследовании значительно ближе к реальным электрофизиологическим экспериментам \cite{ponce2019evolving} и позволяет нам проверять гипотезы о временных паттернах ответов нейронов на НАИ in silico. Мы надеемся, что наши алгоритмы будут полезны в будущем изучении оптимальных стимулов для биологических нейронных сетей.}

\OK{
Наш фреймворк направлен на преодоление разрыва между нейробиологическими и вычислительными экспериментами, предоставляя возможность исследовать принципы кодирования информации в глубоких нейронных сетях. Кроме того, мы надеемся, что это исследование приблизит алгоритмы максимизации активации к требованиям реальных биологических экспериментов, предоставив новый инструмент для анализа когнитивных функций отдельных нейронов.}

\section{Благодарности} 
\OK{
Эта работа была поддержана Некоммерческим фондом поддержки науки и образования «ИНТЕЛЛЕКТ» и Московским государственным университетом имени М.В. Ломоносова. Н. Поспелов выражает благодарность за поддержку программы «Мозг» в Исследовательском центре IDEAS. Основные вычислительные эксперименты проводились в Институте перспективных исследований мозга. В некоторых экспериментах использовались вычислительные ресурсы высокопроизводительных вычислительных средств НИУ ВШЭ \cite{HSE-Supercomputer-counter-paper}. Авторы благодарны всем членам Лаборатории нейронного интеллекта и особенно Ксении Тороповой и Ольге Ивашкиной за плодотворные дискуссии.} 


\appendix

\section{Галерея примечательных НАИ}
\label{appendix:MEI-gallery}

В этом разделе мы представляем некоторые примечательные НАИ для нейронов различных (импульсных) слоев и пытаемся интерпретировать их специализации. 

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{Disser/images/neurotrain/MEI-gallery-0.png}
\caption{НАИ нейронов первого импульсного (sn1) слоя. \textbf{Каждая строка} – определенный нейрон, несколько оптимумов активации.}
\label{nt:MEI-gallery-0}
\end{figure}

Рис.\thinspace\ref{nt:MEI-gallery-0} показывает НАИ нейронов первого импульсного слоя (sn1). Несмотря на импульсную природу нейронов, они явно специализируются на визуальных примитивах – паттернах/текстурах, однородных монохромных «пятнах» – точно так же, как неимпульсные нейроны в ИНС прямого распространения \cite{Bengio-transferable-features,AlexNet}. См. рис.\thinspace\ref{nt:argmax-gen-comp-color},\thinspace\ref{nt:argmax-gen-comp-stripes} в \ref{appendix:VAE} для большего количества примеров НАИ нейронов первого слоя. НАИ становятся более интересными с глубиной слоя.

\begin{figure}[H]
\centering
\includegraphics[width=1.0\textwidth]{Disser/images/neurotrain/MEI-gallery-1.png}
\caption{НАИ нейронов промежуточного (layer2.1.sn1) слоя. \textbf{Каждые ряды из 3 изображений:} один нейрон, несколько оптимумов активации. \textbf{Левый блок 3x3:} 3 разных нейрона с различными специализациями. \textbf{Правый блок 3x3:} 3 разных нейрона с похожими специализациями.}
\label{nt:MEI-gallery-1}
\end{figure}

Рис.\thinspace\ref{nt:MEI-gallery-1} показывает НАИ различных нейронов умеренно глубокого слоя. В левом блоке 3x3 нейрон первой строки специализируется на вытянутых формах на синем фоне: НАИ близки к типичным изображениям классов 8 – корабль и 0 – самолет. Нейрон второй строки специализируется на зеленом низе и синем верхе фона (трава и небо) с объектами между ними (автомобили / грузовики / самолеты на взлетных полосах). Нейрон третьей строки, возможно, наиболее интересен, так как он, по-видимому, специализируется на изображениях, разделяющих признаки птиц и кораблей (зеленая пресная вода) – мы называем его \textbf{нейрон «птица-корабль»}. См. также раздел\thinspace\ref{subsec:emergence-and-dynamics} и рис.\thinspace\ref{nt:labile-neurons-distr-and-bird-ship}.

В правом блоке 3x3 мы демонстрируем 3 \textbf{разных нейрона с очень похожими специализациями}. Очевидно, что НАИ разделяют признаки классов набора данных 7 – лошадь, 4 – олень (возможно, также 5 – собака), но часто оказывались почти зеркально-симметричными, поэтому кажется, что эти нейроны специализируются на таких «\textbf{коричневых четвероногих}» на зеленом (травяном) фоне без особого знания о расположении головы относительно хвоста. В целом ожидается, что умеренно глубокие нейроны специализируются на таких простых типах изображений – более сложных, чем визуальные примитивы, но менее сложных, чем детализированные изображения, четко различимые как классы набора данных. Однако примечательно то, что такие гладкие, но еще не очень специфичные для класса НАИ стабильно получаются с помощью безградиентной процедуры оптимизации, а не простым обратным распространением, как в ИНС прямого распространения \cite{Olah-feature-visualization}.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{Disser/images/neurotrain/MEI-gallery-2.png}
\caption{НАИ последнего импульсного слоя (layer4.1.sn1). \textbf{Каждая строка} – определенный нейрон, несколько оптимумов активации.}
\label{nt:MEI-gallery-2}
\end{figure}

Рисунок\thinspace\ref{nt:MEI-gallery-2} показывает НАИ последнего (самого глубокого) импульсного слоя (layer4.1.sn1). Его нейроны явно специализируются на изображениях, подобных набору данных, с классами 4 – олень, 6 – лягушка, 7 – лошадь – четко различимыми. 


\section{Сравнение генеративных моделей}\label{appendix:VAE}

\subsection{Набор данных CIFAR-10}

Модели были обучены на CIFAR-10 \cite{CIFAR10-dataset-reference} – наборе данных из 60 000 цветных изображений размером 32x32 пикселя, разделенных на 10 классов (6 000 изображений на класс): 0) самолет, 1) автомобиль, 2) птица, 3) кошка, 4) олень, 5) собака, 6) лягушка, 7) лошадь, 8) корабль, 9) грузовик. Рисунок\thinspace\ref{nt:CIFAR10-classes} приводит примеры изображений набора данных.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{Disser/images/neurotrain/CIFAR10-classes.png}
\caption{10 классов изображений в наборе данных CIFAR10 \cite{CIFAR10-dataset-reference}}
\label{nt:CIFAR10-classes}
\end{figure}

\subsection{Обзор VAE}

Помимо SN-GAN, мы также протестировали VQ-VAE для генерации изображений. Векторно-квантованный вариационный автокодировщик (VQ-VAE) \cite{VQ-VAE-Van-Den-Oord} является генеративной моделью семейства VAE \cite{VAE-vanilla-Kingma-Welling}. VAE были выбраны для нашей задачи главным образом из-за быстрой генерации данных – для получения образца необходим только один прямой проход; но также из-за простоты архитектуры – кодировщик-декодировщик, где кодировщик дает латентные координаты образца.

VAE берут образец данных, $x$ (например, пакет изображений) и преобразуют его, слой за слоем кодировщика (если данные являются изображениями, сначала применяются некоторые сверточные слои), в латентное пространство. На этом пространстве определяется семейство параметризованных распределений вероятностей (апостериорных) $p_\theta (z|x)$ (можно сказать, что кодировщик просто выводит вектор параметров $\theta$ этого распределения, таким образом определяя представителя этого семейства). Затем генерируется образец $z\sim p_\theta(z|x)$ из этого распределения – это недетерминированное действие, и поэтому сначала кажется недифференцируемым для разработки обратного прохода (обратного распространения ошибки), но умная идея трюка с репараметризацией \cite{VAE-vanilla-Kingma-Welling} преодолевает эту проблему (разделяя параметризацию распределения от выборки из него). Этот «латентный код» $z$, который описывает входной образец $x$, затем передается в декодировщик (если VAE должен генерировать изображения – последние слои декодировщика будут деконволюционными и т.д.) для создания выхода – нового образца данных. Существует также так называемое априорное распределение $p(z)$ латентных кодов, которое для случая латентного пространства $\mathbb{R}^d$ неинформативно выбирается стандартным нормальным (равномерное распределение не может быть поддержано на $\mathbb{R}^d$). Существует проблема «коллапса апостериорного распределения» – когда апостериорное распределение становится слишком близким к неинформативному априорному на некоторых латентных кодах. Различные модификации VAE направлены на решение этой проблемы, причем VQ-VAE делает это довольно успешно при определенных условиях. 

Векторно-квантованные VAE \cite{VQ-VAE-Van-Den-Oord} переносят эту идею в «дискретную» постановку – семейство апостериорных распределений $p_\theta(z|x)$ теперь дискретно, а не непрерывно (многомерное нормальное в оригинальных VAE), поддерживается на фиксированном по размеру \textbf{словаре латентных кодов} (кодовых слов). Мотивация этого заключается в том, что для любого конечного образца естественных изображений в нем будет только дискретный набор классов – собаки, кошки, автомобили и т.д. Поэтому VQ-VAE, как правило, производят гораздо более четкие изображения, чем их оригинальные аналоги VAE (которые имеют тенденцию размывать изображение, с неопределенностью апостериорного распределения, переходящей в гауссовое, хотя и нелинейно преобразованное, размытие на результирующих изображениях), что сначала казалось лучше для наших нужд. Таким образом, кодировщик VQ-VAE учится выводить эти дискретные апостериорные распределения (латентные коды и их апостериорные вероятности) так, чтобы образцы из них (выход декодировщика) были похожи на обучающие данные. Поскольку априорное распределение $p(z)$ также поддерживается на этой дискретной кодовой базе (которая может поддерживать равномерное распределение), выбор его равномерным помогает предотвратить коллапс апостериорного распределения.  

В наших экспериментах мы протестировали VQ-VAE с \textbf{латентной размерностью 64} и \textbf{размером словаря 512} – хотя CIFAR-10 имеет только 10 классов, коды присваиваются не отдельным изображениям, а их пакетам (образцам). И кодировщик, и декодировщик были построены с использованием ResidualStacks \cite{ResNet-paper} из 3-4 сверточных слоев. 


\subsection{Сравнение генераторов на основе GAN и VAE}

Для НАИ, созданных для каждого нейрона всех импульсных слоев из импульсной ResNet18, анализ показал, что максимальные активации нейронов на изображениях, сгенерированных VQ-VAE, были меньше, чем на изображениях, сгенерированных SN-GAN. Величина разрыва зависела от слоя и варьировалась от 50\% в ранних слоях до 5\% в глубоких слоях.

В целом, несмотря на то, что нейронные активации были значительно выше случайных, НАИ, сгенерированные VAE, часто не содержат видимой структуры и не дают никакого представления о нейронных специализациях.

Исключением являются ранние слои, в которых НАИ, полученные с использованием обоих генераторов, довольно похожи. Это позволяет ожидать, что дискретные методы оптимизации действительно находят \textbf{не зависящие от генератора максимумы} в глобальном пространстве изображений, и трудности в получении интерпретируемых изображений могут быть решены путем дальнейшего улучшения структуры латентного пространства генераторов.
%(Fig. \ref{nt:argmax-gen-comp-color})
\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{Disser/images/neurotrain/am_u16_sn1_merged.png}
\caption{НАИ для единицы 16 первого импульсного слоя из импульсной ResNet18. \textbf{Сверху:} изображения, сгенерированные с помощью SN-GAN. \textbf{Снизу:} изображения, сгенерированные с помощью VQ-VAE. Столбцы соответствуют различным методам оптимизации на основе разложения тензорного поезда.}
\label{nt:argmax-gen-comp-color}
\end{figure}

Таким образом, изображения, сгенерированные VQ-VAE, наиболее близки к более богатым изображениям, сгенерированным SN-GAN, там, где в изображении присутствуют однородные цвета или простые геометрические паттерны (см. рис.\thinspace\ref{nt:argmax-gen-comp-color}, ~\ref{nt:argmax-gen-comp-stripes}). Мы предполагаем, что это признак чрезмерного сжатия информации в латентном пространстве генератора VQ-VAE, что приводит к потере значительных деталей изображения.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{Disser/images/neurotrain/am_u35_sn1_merged.png}
\caption{НАИ для единицы 35 первого импульсного слоя из импульсной ResNet18. \textbf{Сверху:} изображения, сгенерированные с помощью SN-GAN. \textbf{Снизу:} изображения, сгенерированные с помощью VQ-VAE. Столбцы соответствуют различным методам оптимизации на основе разложения тензорного поезда.}
\label{nt:argmax-gen-comp-stripes}
\end{figure}

Наша гипотеза относительно того, почему VQ-VAE работает хуже в нашей постановке: как описано в разделах выше, VQ-VAE изучают дискретизированные, а не непрерывные апостериорные распределения образцов данных – поддерживаемые на конечном множестве (словаре) (изученных) векторов в латентном пространстве, апостериорное распределение затем является мультиномиальным на этом дискретном множестве точек. По этой причине VQ-VAE, по-видимому, не так хороши в интерполяции между вероятными точками данных (изображениями) – разработанные для смягчения коллапса апостериорного распределения и гауссовского размытия сгенерированных изображений в оригинальных VAE, кажется, что VQ-VAE теряют в способности генерировать интересные образцы вне распределения. Конечно, некоторая интерполяция возможна, и кажется, мы не нашли хороший баланс между параметрами дискретизации VQ-VAE и разложения тензорного поезда – это вопрос будущего исследования. 

SN-GAN работали намного лучше в нашем эксперименте, обеспечивая, по-видимому, более гладкие (благодаря преимуществам спектральной нормализации) координаты в латентном пространстве изображений. Однако приведенные выше результаты о распределениях специализации нейронов, по-видимому, универсальны – независимо от того, какая генеративная модель использовалась.

\section{Сравнение динамики оптимальных стимулов в ИНС и импульсных нейронных сетях}\label{appendix:ANN}

Чтобы исследовать, в какой степени полученные результаты определяются импульсной природой исследуемой сети, мы повторили наш анализ на стандартной (неимпульсной) ИНС архитектуры ResNet18. Напомним, что проанализированная нами ИНС была, по сути, версией этой модели, дополненной импульсными слоями. Мы применили методы оптимизации на основе разложения тензорного поезда для выявления оптимальных стимулов для нейронов блоков ResNet18 (всего 8 слоев) во время обучения, а затем применили тот же анализ, что изложен в разделе Результаты, включая отслеживание НАИ на протяжении обучения, к записанным данным. Для этого анализа мы удалили критерий селективности «достаточная активность», поскольку выходная величина неимпульсных нейронов не ограничена сверху. Наши результаты показали значительные различия между процессами формирования НАИ в неимпульсных сверточных сетях и их импульсных аналогах.

\begin{figure}[H]
\centering
\includegraphics[width=0.98\textwidth]{Disser/images/neurotrain/a3.jpg}
\caption{\textbf{Слева:} доля класс-селективных нейронов в модели ИНС. Горизонтальная ось - номер эпохи, вертикальная ось - слои (ниже = глубже). \textbf{Справа:} неограниченные НАИ-связанные активации в модели ИНС по слоям во время обучения. Заштрихованные области указывают стандартные ошибки.}
\label{nt:ann_stats}
\end{figure}

\begin{itemize}
    \item В ИНС доля класс-селективных нейронов растет или остается неизменной во время обучения во всех слоях, в то время как в ИНС этот процесс явно двухэтапный. Начальный рост доли селективных единиц сопровождается снижением во всех слоях, кроме последнего, где она монотонно увеличивается во время обучения (см. рис. \ref{nt:ann_stats}). Кажется, что селективность перетекает в последний слой, а оптимальные стимулы в промежуточных слоях становятся менее утонченными и более специфичными для класса.  

    \item В целом, уверенность сети (измеренная отрицательной энтропией выходов) для сгенерированных НАИ была значительно ниже в версии ИНС ResNet18, чем в импульсной. Единственным слоем со стабильными и надежными связанными с классом НАИ был последний, особенно во время поздних эпох обучения.
\end{itemize}


Учитывая низкую уверенность сети во время инференса на сгенерированных НАИ для неимпульсной ResNet18, возможно, что метод оптимизации на основе разложения тензорного поезда может не работать хорошо в этой задаче. Дальнейший анализ с использованием градиентных методов оптимизации, которые обычно используются для ИНС, может помочь лучше понять эту проблему.
Однако эти результаты могут иметь интригующее объяснение в контексте общей динамики обучения нейронных сетей. Было показано, что забывание (распад информации, содержащейся в весах) имеет решающее значение для достижения инвариантности и разделения в обучении представлений \cite{critical-learning-periods}. Это может объяснить снижение селективности и амплитуд активации в промежуточных слоях. В то же время вычислительная роль последних слоев увеличивается, делая их непропорционально важными для принятия решений моделью, в то время как остальная часть сети служит высокоразмерным «экстрактором признаков» - идея, лежащая в основе резервуарных вычислительных систем \cite{Gauthier2021}.

Эти процессы, по-видимому, также происходят в импульсной сети. Обратите внимание, что максимальные НАИ-связанные активации первых слоев ИНС происходят на промежуточных этапах обучения (см. рис. \ref{nt:activation-dynamics}). Однако импульсная природа нейронов делает обучение сети более сложным, продлевая критические фазы этого процесса. Это предполагает, что можно наблюдать аналогичные эффекты, как в ИНС, если обучать модель достаточно долгое время.


%% If you have bib database file and want bibtex to generate the
%% bibitems, please use
%%
%%  \bibliographystyle{elsarticle-harv} 
%%  \bibliography{<your bibdatabase>}

%% else use the following coding to input the bibitems directly in the
%% TeX file.

%% Refer following link for more details about bibliography and citations.
%% https://en.wikibooks.org/wiki/LaTeX/Bibliography_Management

%\begin{thebibliography}{00}
\bibliography{refs}

%\end{thebibliography}

\end{document}

\endinput
%%
%% End of file `elsarticle-template-harv.tex'.


